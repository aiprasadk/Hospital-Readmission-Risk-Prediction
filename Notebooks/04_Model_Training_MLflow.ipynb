{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb7c0e9-1c82-4af9-b9b5-8b2602c3b746",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clear the Cache"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python memory cleared. Proceed to the next cell.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# Clear specific large variables from memory\n",
    "for var in ['model', 'pipeline', 'rf', 'predictions', 'train_df', 'test_df']:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "\n",
    "gc.collect()\n",
    "print(\"Python memory cleared. Proceed to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ac52ba-4526-45a3-82bb-eb77e4f88e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d252d0c0-c7c5-440e-88dd-cc097fb5aeef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Load and Cast Data\n",
    "# Loading the Gold features table prepared in the previous notebook\n",
    "gold_df = spark.read.table(\"healthcare.gold.readmission_features\")\n",
    "\n",
    "# List of numeric columns to be cast to Double for the VectorAssembler\n",
    "numeric_cols = [\n",
    "    \"time_in_hospital\", \"num_lab_procedures\", \"num_procedures\", \n",
    "    \"num_medications\", \"number_diagnoses\", \"number_outpatient\", \n",
    "    \"number_emergency\", \"number_inpatient\"\n",
    "]\n",
    "\n",
    "for c in numeric_cols:\n",
    "    gold_df = gold_df.withColumn(c, col(c).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf9a53b-e679-4a0f-824e-57dac87755d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Explicitly Define String Indexers\n",
    "# These stages convert string categories into numerical indices with your requested suffix (_idx)\n",
    "age_indexer = StringIndexer(inputCol=\"age\", outputCol=\"age_idx\", handleInvalid=\"keep\")\n",
    "gender_indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_idx\", handleInvalid=\"keep\")\n",
    "race_indexer = StringIndexer(inputCol=\"race\", outputCol=\"race_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "# Start building the pipeline stages list\n",
    "stages = [age_indexer, gender_indexer, race_indexer]\n",
    "\n",
    "# 4. Define Feature Columns List\n",
    "# This is your specific list used for the VectorAssembler\n",
    "feature_cols = [\n",
    "    \"age_idx\",\n",
    "    \"gender_idx\",\n",
    "    \"race_idx\",\n",
    "    \"time_in_hospital\",\n",
    "    \"num_lab_procedures\",\n",
    "    \"num_procedures\",\n",
    "    \"num_medications\",\n",
    "    \"number_outpatient\",\n",
    "    \"number_emergency\",\n",
    "    \"number_inpatient\",\n",
    "    \"number_diagnoses\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d9309e-dc44-446b-b5bd-f685b5050ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Build Vector Assembler\n",
    "# Combines all features into a single 'features' vector for the model\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe79e14-70fe-4851-b174-6b3ff8e773e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Define the UC Volume path for staging (Required for Serverless)\n",
    "uc_volume_path = \"/Volumes/healthcare/bronze/raw_volume/ml_tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa4a96ec-e215-44a4-a1de-261e47458075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/30 17:32:55 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/01/30 17:33:00 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-73426648-1824-48cc-9f79-11/tmp0b47eidm/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/01/30 17:33:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS! Model Training Complete. AUC Score: 0.6186876848159526\n"
     ]
    }
   ],
   "source": [
    "# 7. MLflow Tracking & Model Training\n",
    "experiment_path = f\"/Users/{spark.sql('SELECT current_user()').collect()[0][0]}/Readmission_Prediction\"\n",
    "mlflow.set_experiment(experiment_path)\n",
    "\n",
    "with mlflow.start_run(run_name=\"RF_Final_Integrated\"):\n",
    "    \n",
    "    # Lightweight Classifier for Serverless (10 trees, depth 5)\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10, maxDepth=5)\n",
    "    stages += [rf]\n",
    "    \n",
    "    # Create the Pipeline and Split Data\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    train_df, test_df = gold_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Train the Model\n",
    "    model = pipeline.fit(train_df)\n",
    "    \n",
    "    # Transform test data to get predictions\n",
    "    predictions = model.transform(test_df)\n",
    "    \n",
    "    # Extract Readmission Risk ---\n",
    "    # probability[1] is the likelihood of the patient being readmitted (label 1)\n",
    "    predictions = predictions.withColumn(\n",
    "        \"readmission_risk\", \n",
    "        vector_to_array(col(\"probability\"))[1]\n",
    "    )\n",
    "    \n",
    "    # Evaluate Performance (AUC)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Log metrics and model artifacts to MLflow\n",
    "    mlflow.log_metric(\"auc\", auc)\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=model, \n",
    "        artifact_path=\"readmission_rf_model\",\n",
    "        dfs_tmpdir=uc_volume_path\n",
    "    )\n",
    "    \n",
    "    print(f\"SUCCESS! Model Training Complete. AUC Score: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e19046-92af-4af1-a8ac-1743c479a2e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. FINAL WRITE: Save predictions with exactly 5 columns\n",
    "(\n",
    "    predictions.select(\n",
    "        \"patient_nbr\", \n",
    "        \"label\", \n",
    "        \"prediction\", \n",
    "        \"probability\", \n",
    "        \"readmission_risk\"\n",
    "    )\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\") \n",
    "    .saveAsTable(\"healthcare.gold.readmission_predictions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a3feb8-da1a-4943-ae25-1f038f294685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>patient_nbr</td><td>string</td><td>null</td></tr><tr><td>label</td><td>int</td><td>null</td></tr><tr><td>prediction</td><td>double</td><td>null</td></tr><tr><td>probability</td><td>vector</td><td>null</td></tr><tr><td>readmission_risk</td><td>double</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "patient_nbr",
         "string",
         null
        ],
        [
         "label",
         "int",
         null
        ],
        [
         "prediction",
         "double",
         null
        ],
        [
         "probability",
         "vector",
         null
        ],
        [
         "readmission_risk",
         "double",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "col_name",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "data_type",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "comment",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 11
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE healthcare.gold.readmission_predictions;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8860517564037191,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Model_Training_MLflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}